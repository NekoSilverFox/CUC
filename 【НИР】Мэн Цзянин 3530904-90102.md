<div align="center">
  <img width="250px" src="https://github.com/NekoSilverFox/NekoSilverfox/blob/master/icons/logo_building_spbstu.png?raw=true" align="center" alt="ogo_building_spbstu" />
  </br>
  <b><font size=3>Санкт-Петербургский государственный политехнический университет</font></b>
  </br>
  <b><font size=2>Институт компьютерных наук и технологий</font></b>
  </br>
  <b><font size=2>Высшая школа программной инженерии</font></b>
</div>


</br>
</br>
</br>
</br>

<div align="center">
<b><font size=6>Классификатор блоков кодирования</font></b>
</div>

</br>
</br>
</br>
</br>
</br>


| Выполнил студенты гр. 3530904/90102 | Мэн Ц.            |
| :---------------------------------- | ----------------- |
| Руководитель                        | Черноруцкий И. Г. |

</br>
</br>

<div align="center">
  Санкт-Петербург</br>
  2023
</div>


<div STYLE="page-break-after: always;"></div>

<div align=left>

<p align="center"><b><font size=6>Аннотация</font></b></br></p>

> КЛЮЧЕВЫЕ СЛОВА: МАШИННОЕ ОБУЧЕНИЕ, АЛГОРИТМ КЛАССИФИКАЦИИ, КЛАССИФИКАТОР, ПРЕДСКАЗАНИЕ КЛАССИФИКАЦИИ, КОНТРОЛИРУЕМОЕ ОБУЧЕНИЕ, АЛГОРИТМ, БЛОК КОДИРОВАНИЯ

В данной работе представлен новый алгоритм (Coding unit classification algorithm, CUCL) для решения проблемы предсказания классификации в машинном обучении. CUCL вдохновлен блоком дерева кодирования в высокоэффективное кодирование видеоизображений[^1-1] (High Efficiency Video Coding, HEVC также известное как H.265).

Блок дерева кодирования - это основной блок кодирования HEVC, который поддерживает размеры CTU от 8x8 до 64x64 пикселей. Вместо использовавшихся ранее макроблоков 16 x 16 пикселей, блок Coding Tree Unit может использоваться в больших блочных структурах 64 x 64 и позволяет лучше подразделять изображения на переменные размеры. Изображение изначально делится на ячейки дерева кодирования, которые могут быть 64×64, 32×32 или 16×16, причем увеличение размера блока пикселей обычно повышает эффективность кодирования.

Используя идею единиц кодирования в дереве кодирования, мы разработали классификатор блоков кодирования (Coding unit classifier, CUC), а также предложили новый метод выявления и удаления выбросов из набора данных.

В данной работе используется библиотека scikit-learn, а в качестве сред разработки - PyCharm и Visual Studio Code. Язык разработки - Python3.

*Экспериментальное исследование* будет проведено на ряде наборов данных в окончательных результатах для подтверждения эффективности предложенного нами алгоритма. Все экспериментальные исследования мы проводили на устройстве, оснащенном платформой macOS13, которая имеет 64-битную операционную систему, 8-ядерный процессор Apple M1 Pro и 16 ГБ оперативной памяти на унифицированной архитектуре памяти.



[toc]

# Введение

Машинное обучение - это направление искусственного интеллекта. История искусственного интеллекта имеет естественную и четкую последовательность от фокуса на "рассуждениях", к фокусу на "знаниях", к фокусу на "обучении". Очевидно, что машинное обучение - это способ достижения искусственного интеллекта, то есть использование машинного обучения как средства для решения задач искусственного интеллекта. За последние 30 лет машинное обучение превратилось в междисциплинарную дисциплину, включающую теорию вероятностей, статистику, теорию аппроксимации, выпуклый анализ, теорию сложности вычислений и многие другие дисциплины. Теория машинного обучения сосредоточена на разработке и анализе алгоритмов, которые позволяют компьютерам "обучаться" автоматически. Алгоритмы машинного обучения - это класс алгоритмов, которые автоматически анализируют данные для получения закономерностей и используют эти закономерности для составления прогнозов относительно неизвестных данных. Поскольку алгоритмы обучения включают в себя большое количество статистической теории, машинное обучение особенно тесно связано с инференциальной статистикой и также известно как теория статистического обучения. С точки зрения разработки алгоритмов, теория машинного обучения занимается алгоритмами обучения, которые можно реализовать и которые работают. Многие проблемы инференции попадают в категорию процедурно несложных, поэтому частью исследований в области машинного обучения является разработка алгоритмов аппроксимации, с которыми легко справиться. [^1-2]

Машинное обучение широко используется в таких областях, как поиск данных, компьютерное зрение, обработка естественного языка, биометрическое распознавание, поисковые системы, медицинская диагностика, обнаружение мошенничества с кредитными картами, анализ фондового рынка, секвенирование последовательности ДНК, распознавание речи и рукописного текста, игры и робототехника.



**Машинное обучение можно разделить на следующие категории:**

- Контролируемое обучение изучает функцию на основе заданного набора обучающих данных, на основе которой можно предсказать результаты при поступлении новых данных. Требование к обучающему множеству для контролируемого обучения заключается в том, чтобы оно включало в себя как входы, так и выходы, которые также могут быть описаны как признаки и цели. Цели в обучающем наборе маркируются человеком. Общие алгоритмы контролируемого обучения включают регрессионный анализ и статистическую классификацию.

    Разница между контролируемым и неконтролируемым обучением заключается в том, аннотированы ли цели в обучающем множестве человеком. Они оба имеют обучающее множество и оба имеют входы и выходы

- В отличие от контролируемого обучения, неконтролируемое обучение не имеет аннотированных человеком результатов в обучающем наборе. К распространенным алгоритмам обучения без наблюдения относятся генеративные состязательные сети (GAN) и кластеризация.

- Полусамостоятельное обучение является промежуточным между самоконтролем и несамостоятельным обучением.

- Машина с дополненным обучением постепенно корректирует свое поведение в ответ на изменения в окружающей среде для достижения своих целей и оценивает, является ли обратная связь, полученная после каждого действия, положительной или отрицательной.



Для решения задач классификационного прогнозирования в машинном обучении было предложено много алгоритмов с хорошими результатами（T.Cover, P.Hart, 1967[^1-3]; Harry Zhang, 2004[^1-4]; Kevin P. Murphy, 2006[^1-5]; Leo Breiman, 1984[^2-5]; Quinlan, J. R. 1986[^2-6]; Quinlan, J. R. 1993[^2-7]; Freund, Y., Mason, L. 1999[^2-9]; Tin Kam Ho, 1995[^2-11], Leo Breiman, 2001[^2-12]; David Cox, 1958[^2-22]; V.N. Vapnik，A.Y. Chervonenkis，C. Cortes и др., (1964)[^2-14]）。Некоторые из этих алгоритмов обсуждаются в следующей главе.



# Обзор литературы

Прежде чем представить алгоритм, предложенный в данной статье, мы расскажем о некоторых предыдущих работах, выполненных в области машинного обучения для классификационного прогнозирования.

- В 1967 году T.M. COVER и P.E. HART предложили алгоритм kNN (k-NearestNeighbor, kNN) [^1-2]. Где k относится к k-ближайшим соседям данной выборки, что означает, что каждая выборка может быть представлена или предсказана ее k-ближайшими соседями. Основная идея алгоритма kNN заключается в том, что если большинство из k ближайших соседей образца в пространстве признаков принадлежат к определенному классу, то данный образец также принадлежит к этому классу и обладает свойствами образцов этого класса. Метод полагается только на категорию ближайшего образца или образцов для определения категории, к которой принадлежит классифицируемый образец с точки зрения определения классификационного решения.

    
  
- В (Harry Zhang, 2004[^1-3]; Kevin P. Murphy, 2006[^1-4]) предложен простой байесовский классификатор. Эта модель классификатора присваивает экземплярам задачи метки классов, представленные значениями признаков, причем метки классов берутся из конечного множества. Это не один алгоритм обучения таких классификаторов, а серия алгоритмов, основанных на одном и том же принципе: все простые байесовские классификаторы предполагают, что каждый признак выборки некоррелирован с каждым другим признаком. Будучи широко изученным с 1950-х годов, простой Байес был введен в сообщество текстового информационного поиска в начале 1960-х годов под другим названием [^2-1] и остается популярным методом для классификации текста, который представляет собой проблему определения категории или другой категории (например, спам, законность, спорт или политика, и т.д.), к которой принадлежит документ, используя частоту слов в качестве признака. При соответствующей предварительной обработке он может конкурировать с более продвинутыми методами в этой области (включи метод опорных векторов). Он также находит применение в автоматизированной медицинской диагностике.

    
  
- Деревья решений - это собирательное название класса моделей деревьев, которое появилось примерно в 1948 году, когда Клод Шеннон представил теорию информации [^2-2], одну из теоретических основ обучения с помощью деревьев решений. Затем, в 1963 году, Морган и Сонквист разработали первое дерево регрессии [^2-3], в котором предложили метод анализа данных опроса, не накладывающий ограничений на эффекты взаимодействия, ориентированный на снижение ошибок прогнозирования, действующий последовательно и не зависящий от степени линейности классификации или порядка расположения объясняющих переменных, который они назвали в то время моделью обнаружения взаимодействия (AID). Поскольку AID не учитывал вариабельность выборки, присущую данным, Мессенджер и Манделл в 1972 году предложили дерево THAID, чтобы заполнить этот пробел, а в 1980 году Гордон В. Касс разработал алгоритм CHAID [^2-4], формальное расширение процедур AID и THAID.
    Затем появились CART-деревья - Classification and Regression Trees (сокращенно CART) - термин, введенный Лео Брейманом[^2-5] для обозначения алгоритмов деревьев решений, используемых для решения задач классификации или регрессионного моделирования прогнозов.CART-модели включает в себя выбор входных переменных и разбиение точек на эти переменные до тех пор, пока не будет создано соответствующее дерево. Жадный алгоритм используется для выбора входных переменных и точек разделения, чтобы минимизировать функцию стоимости (cost function).
    
    В 1986 году Куинлан разработал алгоритм ID3 [^2-6], а в последующие годы предложил алгоритм C4.5 [^2-7]. Позже, чтобы улучшить его вычислительную эффективность, Лох и Ших разработали QUEST [^2-8] в 1997 году.
    В 1999 году Йоав Фройнд и Ллью Мейсон предложили AD-дерево [^2-9], которое до этого момента уже достигло хороших успехов в плане корректности, но в то время дерево решений все еще было слишком большим по структуре и не простым для понимания, поэтому в AD-дерево были внесены изменения в узлы. Алгоритм голосует не только по числу, но и по узлам принятия решений. И каждый узел принятия решения имеет семантическую информацию, поддерживающую графическое представление и облегчающую его понимание.

    Популярные в настоящее время алгоритмы дерева решений включают ID3, CHAID, CART, QUEST и C4.5.

    Дерево решений можно рассматривать как набор правил "если - то", т.е. для каждого пути от корня к листовым узлам дерева решений строится правило, признаки внутренних узлов на пути соответствуют условиям правила, а классы листовых узлов соответствуют выводам правила. Таким образом, дерево решений можно рассматривать как состоящее из условия if (внутренний узел) и правила then (ребро), соответствующего выполняемому условию.

    Деревья решений работают путем итеративного разделения данных на различные подмножества в жадной манере (greedy). Дерево регрессии (regression tree) стремится минимизировать MSE (среднюю квадратичную ошибку) или MAE (среднюю абсолютную ошибку) во всех подмножествах, а дерево классификации (classification tree) разбивает данные таким образом, чтобы полученное подмножество имело наименьшую энтропию или примесь Джини (Gini impurity).[^2-10]

    
    
- Случайные леса были предложены Тин Кам Хо в 1995 году [^2-11]. Метод построения леса некоррелированных деревьев с помощью CART-подобного процесса, сочетающего оптимизацию случайных узлов и bagging, был впоследствии предложен в работе Лео Бреймана в 2001 году [^2-12]. Кроме того, в данной работе объединен ряд известных и новых компонентов, составляющих основу современной практики случайного леса, в частности

    1. использование ошибок вне мешка вместо ошибок обобщения (out-of-bag)
    2. важность переменных с помощью метрик ранжирования

    Случайные леса также являются разновидностью метода интегрированного обучения. В машинном обучении случайный лес - это классификатор, который содержит несколько деревьев решений и чей выход определяется множеством классов отдельных выходов деревьев. Модели RF были показаны как надежные предсказатели для небольших выборок и данных высокой размерности (Biau и Scornet, 2016 [^2-13]).



- Классификатор логистической регрессии (Logistic regression, LR) - еще один базовый метод, который, несмотря на свое название, является "классификатором", а не методом "регрессии", первоначально предложенный Дэвидом Коксом в 1958 году [^2-22], и который строит Логистическая модель (также известная как модель Logit). Его наиболее заметным преимуществом является то, что он может использоваться как для классификации, так и для оценки вероятности категорий, поскольку он связан с логистическим распределением данных. Он берет линейную комбинацию признаков и применяет к ним нелинейную sigmoid функцию. В базовой версии логистической регрессии выходная переменная является бинарной, однако ее можно расширить до нескольких категорий (тогда она называется мультиномиальной логистической регрессией (multinomial logistic regression)). Если бинарная логистическая модель делит выборку на два класса, то мультиномиальная логистическая модель расширяет ее на любое количество классов без их сортировки. [^2-23]

    

- Метод опорных векторов (Support Vector Machine, SVM) была предложена В.Н. Вапником, А.Ю. Червоненкисом, К. Кортесом и др. в 1964 году [^2-14], а в 1964 году Вапник и Алексей Ю. Червоненкис составили обобщенный алгоритм портрета дальнейшего обсуждения и установили линейную SVM с жесткими полями [^2-15]. За этим в 1970-х и 1980-х годах последовали теоретические исследования границы принятия решения с максимальным запасом в распознавании образов [^2-16], появление методов решения задач планирования на основе переменной слабины [^2-17] и введение измерения VC (Vapnik-Chervonenkis dimension, измерение VC) [^2-18]. В 1992 году Бернхард Э. Бозер, Изабель М. Гион и Вапник получили нелинейные SVM с помощью ядерных методов [^2-19]. в 1995 году Коринна Кортес и Вапник предложили нелинейную SVM с мягкими полями и применил ее к задаче распознавания рукописных символов [^2-20].

    SVM - это класс обобщенных линейных классификаторов (generalized linear classifier), которые выполняют двоичную классификацию данных в режиме контролируемого обучения, где граница принятия решения - это гиперплоскость с максимальной погрешностью, решенная для изученных образцов (maximum-margin hyperplane) [^2-21]. SVM - это разреженный и надежный классификатор, который использует функцию потерь петля (hinge loss) для расчета эмпирического риска (empirical risk) и добавляет член регуляризации в систему решений для оптимизации структурного риска (structural risk).
    
    SVM может выполнять нелинейную классификацию с помощью метода ядра (kernel method), который является одним из распространенных ядерных методов обучения (kernel learning).
    



# Список литературы

[^1-1]:[Overview of the High Efficiency Video Coding (HEVC) Standard](https://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf)
[^1-2]: [Machine Learning, wikipedia](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)
[^1-3]:[T.Co ver and P. Hart (1967). "Nearest neighbor pattern classification" in IEEE Transactions on Information Theory, vol.13, no.1, pp.21-27, doi: 10.1109/TIT.1967.1053964.](https://ieeexplore.ieee.org/abstract/document/1053964)
[^1-4]:[Harry Zhang (2004). The Optimality of Naive Bayes](http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)
[^1-5]:[Kevin P. Murphy (2006). Naive Bayes classifiers](https://www.ic.unicamp.br/~rocha/teaching/2011s1/mc906/aulas/naive-bayes.pdf)

[^2-1]:Russell, Stuart; Norvig, Peter. Artificial Intelligence: A Modern Approach. 2nd. Prentice Hall. 2003 [1995]. ISBN 978-0137903955
[^2-2]:Shannon, C. E. (1949). Communication Theory of Secrecy Systems. Bell System Technical Journal 28 (4): 656–715.
[^2-3]:Morgan. J. N. & Sonquist, J. A. (1963) Problems in the Analysis of Survey Data, and a Proposal, Journal of the American Statistical Association, 58:302, 415-434.
[^2-4]:Gordon V. K. (1980).An  Exploratory Technique for Investigating Large Quantities of Categorical Data, Applied Statistics. 29(2): 119–127.
[^2-5]:Breiman, L.; Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and regression trees. Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software.
[^2-6]:Quinlan, J. R. (1986). Induction of Decision Trees. Mach. Learn. 1(1): 81–106
[^2-7]:Quinlan, J. R. (1993). C4.5: Programs for machine learning. San Francisco,CA: Morgan Kaufman.
[^2-8]:Loh, W. Y., & Shih, Y. S. (1997). Split selection methods for classification trees. Statistica sinica, 815-840.
[^2-9]:Freund, Y., & Mason, L. (1999, June). The alternating decision tree learning algorithm. In icml (Vol. 99, pp. 124-133).
[^2-10]:[Mos Zhang, Yuanyuan Li (2022). Decision tree learning](https://www.jiqizhixin.com/graph/technologies/80fbc146-bc42-4585-93e3-21c2dd5ca63f).
[^2-11]:Tin Kam Ho. [Random decision forests](https://web.archive.org/web/20210303185509/https://ieeexplore.ieee.org/document/598994/). Proceedings of 3rd International Conference on Document Analysis and Recognition (Montreal, Que., Canada: IEEE Comput. Soc. Press). 1995, **1**: 278–282 [2020-03-04]. [ISBN 978-0-8186-7128-9](https://zh.m.wikipedia.org/wiki/Special:网络书源/978-0-8186-7128-9). [doi:10.1109/ICDAR.1995.598994](https://dx.doi.org/10.1109%2FICDAR.1995.598994).
[^2-12]:Leo Breiman (2001).  Random forests.
[^2-13]:Biau, G., Scornet, E. A (2016). random forest guided tour. *TEST* **25**, 197–227. https://doi.org/10.1007/s11749-016-0481-7
[^2-14]:Vapnik, V.N. and Lerner, A.Y., 1963. Recognition of patterns with help of generalized portraits. Avtomat. i Telemekh, 24(6), pp.774-780.
[^2-15]:Vapnik, V. and Chervonenkis, A., 1964. A note on class of perceptron. Automation and Remote Control, 24.
[^2-16]:Cover, T.M., 1965. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, (3), pp.326-334.
[^2-17]:Cover, T.M., 1965. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, (3), pp.326-334.
[^2-18]:Vapnik, V.N. and Chervonenkis, A.Y., 2015. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of complexity (pp. 11-30). Springer, Cham.
[^2-19]:Boser, B.E., Guyon, I.M. and Vapnik, V.N., 1992, July. A training algorithm for optimal margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory (pp. 144-152). ACM.
[^2-20]:Cortes, C. and Vapnik, V., 1995. Support-vector networks. Machine learning, 20(3), pp.273-297.
[^2-21]:Li Hang．Statistical Learning Methods. Beijing: Tsinghua University Press, 2012: Chapter 7, pp. 95-135
[^2-22]:[Cox, D.R. (1958) The Regression Analysis of Binary Sequences. Journal of the Royal Statistical Society: Series B, 20, 215-242.](https://www.jstor.org/stable/2983890)
[^2-23]:[Stephanie Kay Ashenden (2021). The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry.](https://www.sciencedirect.com/book/9780128200452/the-era-of-artificial-intelligence-machine-learning-and-data-science-in-the-pharmaceutical-industry#book-info)



























